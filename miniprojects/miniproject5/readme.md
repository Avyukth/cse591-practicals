Mini Project 5
--------------

This is an independent project.  

In the yann toolbox, you can supply initial parameters to the ``add_layer`` method using the 
argument ``input_params``. It can also take the argument ``learnable`` which is ``True`` by default.
If ``learnable`` was ``False``, those parameters would be considered as frozen and will not be 
updated during backprop. Using these two options, and the three datasets provided through the 
generators in the ``datasets.py`` you will study the generality of these datasets on the softmax
layer.

To save a network's parameters down, you may use the following commands:

    from yann.utils.pickle import pickle
    pickle(net, 'network.pkl)

To load the network parameters as a dictionary you can run the following command:

    from yann.utils.pickle import load
    params = load('network.pkl')

The ``params`` is now a dictionary with the keys being the layer ids and each value being the list 
of parameters that could be supplied to the ``add_layer`` method. 

    
Using all these tools you will perform the following generality experiments:

1. Train a network (on a base dataset (one of the three).
2. Save the network down and note its performance down 
    
    .. image:: https://latex.codecogs.com/gif.latex?$\Psi(D_i|r)$" title="$\Psi(D_i|r)$

3. Load the parameters and create a new network which uses the parameters of the base layer
    with all the layers but the softmax layer frozen (``learnable = False``).
4. Train only the softmax layer for this network on a re-train dataset (one of the remaining 
    two). 
5. Note the performance down $\Psi(D_j|D_i)$.

The generality of the dataset $D_i$ with respect to $D_j$ is $\Psi(D_j|D_i) / \Psi(D_i|r)$. Using 
this technique measure dataset generality of all the dataset with respect to all the other datasets
among those provided in the ``datasets.py`` file. The network you will use to train them is a two
convolutional, two dense layer network. The first layer of the network has 20 neurons, of 5X5 with a 
pooling of 2X2. The second is a 50 neurons of 3X3 with a pooling of 2X2. The third and fourth are 
dot product layers with 800 nodes each and ``dropout_rate = 0.5``. Apply a $L1$ and $L2$ 
co-efficient on all layers of $0.0001$. Use RMSPROP with Nesterov momentum. The other properties
are upto you. 


The submission for this project is a zip file containing a screen shot of the test output of the
base tutorial code, your codes for your best setting and a one-page report. The one-page report will
be typeset in the [camera-ready](https://www.computer.org/web/tpami/author)
style of IEEE TPAMI.

The report should contain results, analysis and discussion comparing the three network models 
just learnt, difficultiesfaced during these implementations and solutions arrived at for each of 
these difficulties. You will use the mnist dataset generated by cook_mnist_normalized method in 
yann.utils.


Installation
------------

For the yann toolbox installation and other setup details refer the 
[yann toolbox documentation](http://www.yann.network).
